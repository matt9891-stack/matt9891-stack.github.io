---
layout: post
title: "Unit 5 Data Visualisation Seminar Activity"
subtitle: "Plotly Data Visualisation"
date: 2026-02-28
categories: [Module 4 Visualising Data]
tags: [Python, Plotly]
---
This e-portfolio demonstrates the application of regression techniques and machine learning models to explore and predict trends in real-world datasets. This artefact reproduces and extends the examples presented on the Plotly documentation page on ML regression (Plotly, 2026).
Using Python libraries such as Plotly, Seaborn, Matplotlib, Statsmodels, and Scikit-learn, as well as interactive tools like Dash, we investigate linear relationships, non-linear patterns, polynomial regression, and model generalization. Each section includes the code used, a simple explanation, what the plot shows, and a brief critical conclusion. 
The goal is to illustrate not only how regression models can be applied, but also how interactive visualisation and model evaluation improve interpretability and insight.
# 1. Loading Data and Checking It

We begin by loading the “tips” dataset from Plotly Express and examining its structure, including the number of rows and columns, data types, and missing values. 
The dataset contains 244 observations with a mix of numeric and categorical columns and no missing values. This ensures the data is clean and ready for regression analysis.

    import pandas as pd
    import plotly.express as px

### Load dataset
    df = px.data.tips()

### Check basic info
    df.info()
    print('Shape:', df.shape)  
    print(df.dtypes)
    print('Null values:', df.isnull().sum())

    SHAPE
    the dataset has 244 rows and 7 columns 

    DTYPES
    The column total_bill is a float64
    The column tip is a float64
    The column sex is a object
    The column smoker is a object
    The column day is a object
    The column time is a object
    The column size is a int64

    NULL VALUES
    The column total_bill has the 0.0 % of null values
    The column tip has the 0.0 % of null values
    The column sex has the 0.0 % of null values
    The column smoker has the 0.0 % of null values
    The column day has the 0.0 % of null values
    The column time has the 0.0 % of null values
    The column size has the 0.0 % of null values

We begin by loading the “tips” dataset and checking its structure. The dataset contains 244 observations and seven columns with no missing values, making it ready for regression analysis. This step ensures we understand the type and quality of the data before modeling.

Conclusion: The dataset is clean and balanced, containing both numeric and categorical variables suitable for regression.

# 2. Simple Regression with Plotly

        import plotly.express as px

        fig = px.scatter(df, x='total_bill', y='tip', trendline='ols', trendline_color_override='darkblue')
        fig.show()

<img width="2158" height="450" alt="image" src="https://github.com/user-attachments/assets/06d8060b-2de2-4b03-85c4-6044a8565608" />

Fig 1 Scatterplot with Linear Regression

A scatter plot of total_bill versus tip is generated, with a linear regression line overlaid using Plotly’s trendline feature. 
The scatter points represent individual bills and tips, while the line shows the predicted tip for each total bill. 

The plot demonstrates a roughly linear relationship between total bill and tip, suggesting that linear regression is appropriate.

# 3. Linear Regression with Statsmodels

Using statsmodels, we fit a simple linear regression model to predict tips from total bill amounts. The intercept is added to account for the baseline tip.

    import statsmodels.api as sm

    X = sm.add_constant(df['total_bill'])
    y = df['tip']

    model = sm.OLS(y, X).fit()
    print(model.summary())

    OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                    tip   R-squared:                       0.457
    Model:                            OLS   Adj. R-squared:                  0.454
    Method:                 Least Squares   F-statistic:                     203.4
    Date:                Sat, 28 Feb 2026   Prob (F-statistic):           6.69e-34
    Time:                        14:36:57   Log-Likelihood:                -350.54
    No. Observations:                 244   AIC:                             705.1
    Df Residuals:                     242   BIC:                             712.1
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          0.9203      0.160      5.761      0.000       0.606       1.235
    total_bill     0.1050      0.007     14.260      0.000       0.091       0.120
    ==============================================================================
    Omnibus:                       20.185   Durbin-Watson:                   2.151
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):               37.750
    Skew:                           0.443   Prob(JB):                     6.35e-09
    Kurtosis:                       4.711   Cond. No.                         53.0
    ==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

The model summary reveals that the coefficient for total bill is approximately 0.105, indicating that each additional dollar in the total bill increases the tip by around 10 cents. 
The R² value of 0.46 suggests that the model explains about 46% of the variability in tips. This indicates that while the relationship is moderate, other factors may also influence tipping behavior.

# 4. Visualising Regression with Matplotlib and Seaborn

    res=model.predict(X)

    sns.scatterplot(data=df,x='total_bill',y='tip')
    plt.plot(df['total_bill'],res,color = 'r');

<img width="562" height="433" alt="image" src="https://github.com/user-attachments/assets/14f21ab6-316c-41f8-9cac-b10e18d8f9ac" />

Fig 2 Linear Regression using Matplotlib and Seaborn    

We visualise the regression results by plotting the actual data points and overlaying the predicted regression line.
The red line represents the model’s predictions, while the scatter points represent the actual tips. 

The visualisation shows that the regression line captures the upward trend, though it does not perfectly explain all the variability, which is expected given the moderate R².

5. Linear Regression with Scikit-learn

        from sklearn.linear_model import LinearRegression
        from sklearn.model_selection import train_test_split

        X1 = df[['total_bill']]
        y1 = df['tip']

        X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)
        model1 = LinearRegression()
        model1.fit(X_train, y_train)

        pred = model1.predict(X_test)
        X_test['predict'] = pred

        sns.scatterplot(data=df, x='total_bill', y='tip')
        sns.lineplot(data=X_test, x='total_bill', y='predict', color='red')
        plt.show()

<img width="562" height="433" alt="image" src="https://github.com/user-attachments/assets/74128a4b-f7f0-4e6f-b1b0-d37e3a36bfaa" />

Fig 2 Linear Regression Model using Scikit-Learn

We also perform the regression using scikit-learn. The dataset is split into training and testing sets, and the model is fitted to the training data. Predictions are made on the test set, and the regression line is plotted over the data points. 
The plot confirms that the model captures the linear trend between total bill and tip, consistent with the statsmodels results, and demonstrates how sklearn facilitates an ML workflow.

# 6. Interactive Regression with Dash
     ```python
      from dash import Dash, dcc, html, Input, Output
      from sklearn.model_selection import train_test_split
      from sklearn import linear_model, tree, neighbors
    import plotly.graph_objects as go
    import plotly.express as px
    import numpy as np

    app = Dash(__name__)   
    models = {
    'Regression': linear_model.LinearRegression,
    'Decision Tree': tree.DecisionTreeRegressor,
    'k-NN': neighbors.KNeighborsRegressor
    }

    app.layout = html.Div([
        html.H4("Predicting restaurant's revenue"),
        html.P("Select model:"),
    dcc.Dropdown(
        id='dropdown',
        options=["Regression", "Decision Tree", "k-NN"],
        value='Decision Tree',
        clearable=False
    ),
    dcc.Graph(id="graph")
    ])

    @app.callback(
        Output("graph", "figure"),
        Input("dropdown", "value")
    )
    def train_and_display(name):
        df = px.data.tips()
        X = df.total_bill.values[:, None]
        y = df.tip
        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
    
    model2 = models[name]()
    model2.fit(X_train, y_train)
    
    x_range = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
    y_range = model2.predict(x_range)
    
    fig = go.Figure([
        go.Scatter(x=X_train.squeeze(), y=y_train, name='train', mode='markers'),
        go.Scatter(x=X_test.squeeze(), y=y_test, name='test', mode='markers'),
        go.Scatter(x=x_range.squeeze(), y=y_range, name='prediction')
    ])
    return fig

      app.run(debug=True)```

<img width="2159" height="450" alt="newplot2" src="https://github.com/user-attachments/assets/24d96fd3-bb11-4607-923a-a2cea1c0b1b6" />

Fig 3 Dynamic Plot

An interactive Dash app allows comparison of three models: linear regression, decision tree, and k-Nearest Neighbours (kNN). 
The user can select the model from a dropdown menu, and the plot updates dynamically. Scatter plots display training and testing points, while the prediction line or curve shows the model’s fit. 
This interactive visualisation helps users understand the differences between linear and non-linear models.

# 7. k-Nearest Neighbours Regression Comparison
   
    ```python
      from sklearn.neighbours import KNeighborsRegressor
      
      X = df.total_bill.values.reshape(-1,1)
      x_range = np.linspace(X.min(), X.max(), 100)
      
      knn_dist = KNeighborsRegressor(10, weights='distance')
      knn_dist.fit(X, df.tip)
      y_dist = knn_dist.predict(x_range.reshape(-1,1))
      
      knn_uni = KNeighborsRegressor(10, weights='uniform')
      knn_uni.fit(X, df.tip)
      y_uni = knn_uni.predict(x_range.reshape(-1,1))
      
      fig = px.scatter(df, x='total_bill', y='tip', color='sex', opacity=0.65)
      fig.add_traces(go.Scatter(x=x_range, y=y_uni, name='Weights: Uniform'))
      fig.add_traces(go.Scatter(x=x_range, y=y_dist, name='Weights: Distance'))
      fig.show()

The dataset is also modelled using kNN regression, which predicts values based on nearby neighbours. Two variations are compared: uniform weighting, where each neighbour contributes equally, and distance weighting, where closer neighbours have more influence. 
The plot shows two curves corresponding to the two weighting methods. This demonstrates how kNN can capture non-linear patterns that linear regression cannot, and distance weighting tends to follow local variations more closely.

<img width="3219" height="663" alt="image" src="https://github.com/user-attachments/assets/e1f9489f-0ef1-4802-8f07-5f2de55bad39" />

Fig 4 Scatter Plot of the KNN model result

# 8. Polynomial Regression
    ```python
      from sklearn.preprocessing import PolynomialFeatures
      
      def format_coefs(coefs):
          equation_list = [f"{coef}x^{i}" for i, coef in enumerate(coefs)]
          equation = "$" + " + ".join(equation_list) + "$"
          replace_map = {"x^0": "", "x^1": "x", "+ -": "- "}
          for old, new in replace_map.items():
              equation = equation.replace(old, new)
          return equation
      
      X = df.total_bill.values.reshape(-1,1)
      x_range = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
      
      fig = px.scatter(df, x='total_bill', y='tip', opacity=0.65)
      for degree in [1,2,3,4]:
          poly = PolynomialFeatures(degree)
          X_poly = poly.fit_transform(X)
          x_range_poly = poly.transform(x_range)
          
          model = LinearRegression(fit_intercept=False)
          model.fit(X_poly, df.tip)
          y_poly = model.predict(x_range_poly)
          
          equation = format_coefs(model.coef_.round(2))
          fig.add_traces(go.Scatter(x=x_range.squeeze(), y=y_poly, name=equation))
      fig.show()

 <img width="3236" height="671" alt="image" src="https://github.com/user-attachments/assets/6b17dfa9-f45e-4e95-aa5c-d2296603858f" />
 
 Fig 5 Polynomial Reg Plot
     
Linear regression is extended to polynomial regression using scikit-learn’s PolynomialFeatures. 
Models of degrees 1 to 4 are fitted to the data. The resulting curves are overlaid on the scatter plot, showing how increasing the polynomial degree allows the model to capture more complex trends. 
While higher-degree polynomials can closely follow the data, they may also overfit, so a balance between complexity and generalisation is necessary.

# 9. 3D Regression Surface

    from sklearn.svm import SVR
    import numpy as np
    
    df = px.data.iris()
    X = df[['sepal_width','sepal_length']]
    y = df['petal_width']
    
    model = SVR(C=1.)
    model.fit(X, y)
    
    x_min, x_max = X.sepal_width.min(), X.sepal_width.max()
    y_min, y_max = X.sepal_length.min(), X.sepal_length.max()
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    pred = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    fig = px.scatter_3d(df, x='sepal_width', y='sepal_length', z='petal_width')
    fig.add_traces(go.Surface(x=np.arange(x_min, x_max, 0.02), y=np.arange(y_min, y_max, 0.02), z=pred, name='pred_surface'))
    fig.show()
    
<img width="2371" height="601" alt="image" src="https://github.com/user-attachments/assets/28e05e53-2844-4eaf-ac50-b7d49a6a2275" />

Fig 6 3D Plot

Using a Support Vector Regressor, we predict petal_width from sepal_length and sepal_width. A 3D scatter plot displays the actual observations, and a surface represents the predicted values. 
This visualisation shows how a model can handle multiple features and produce smooth, non-linear predictions, providing a clear view of the relationship between the inputs and the target variable.

# 10. Multiple Linear Regression Coefficients

    X = pd.get_dummies(df.drop(columns=['petal_width','species_id']), columns=['species'], prefix_sep='=')
    y = df['petal_width']
    
    model = LinearRegression()
    model.fit(X, y)
    
    colors = ['Positive' if c>0 else 'Negative' for c in model.coef_]
    fig = px.bar(x=X.columns, y=model.coef_, color=colors, color_discrete_sequence=['red','blue'],
                 labels={'x':'Feature','y':'Linear coefficient'}, title='Weight of each feature')
    fig.show()

<img width="3221" height="661" alt="image" src="https://github.com/user-attachments/assets/40f2136c-917b-4ce0-baae-98d421125c73" />

Fig 7  Coefficients plot

We apply multiple linear regression to predict petal_width, including both numeric features and one-hot-encoded categorical variables. 
A bar chart visualises the coefficients, with positive values in blue and negative values in red. The plot highlights the impact of each feature on the target, making it easy to interpret which variables increase or decrease petal width.

# 11. Actual vs Predicted and Residual Plots

    df = px.data.iris()
    X = df[['sepal_width', 'sepal_length']]
    y = df['petal_width']
    
    # Condition the model on sepal width and length, predict the petal width
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)
    
    fig = px.scatter(x=y, y=y_pred, labels={'x': 'ground truth', 'y': 'prediction'})
    fig.add_shape(
        type="line", line=dict(dash='dash'),
        x0=y.min(), y0=y.min(),
        x1=y.max(), y1=y.max()
    )
    fig.show()

<img width="3226" height="669" alt="image" src="https://github.com/user-attachments/assets/c616e19c-bb0a-465b-a88e-14ccf24084b9" />

Fig 8 Predicted v Actual

Scatter plots comparing predicted versus actual values are created, along with residual plots showing the differences between predictions and true values. 
Points close to the y = x line indicate accurate predictions, while residual plots highlight any patterns or outliers. These plots are useful for diagnosing model performance and identifying where the model may not fit well.

12. Regularisation with LassoCV

        from sklearn.linear_model import LassoCV
        from sklearn.preprocessing import StandardScaler
        
        N_FOLD = 6
        
        # Load and preprocess the data
        df = px.data.gapminder()
        X = df.drop(columns=['lifeExp', 'iso_num'])
        X = pd.get_dummies(X, columns=['country', 'continent', 'iso_alpha'])
        y = df['lifeExp']
        
        # Normalize the data
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Train model to predict life expectancy
        model = LassoCV(cv=N_FOLD)
        model.fit(X_scaled, y)
        mean_alphas = model.mse_path_.mean(axis=-1)
        
        fig = go.Figure([
            go.Scatter(
                x=model.alphas_, y=model.mse_path_[:, i],
                name=f"Fold: {i+1}", opacity=.5, line=dict(dash='dash'),
                hovertemplate="alpha: %{x} <br>MSE: %{y}"
            )
            for i in range(N_FOLD)
        ])
        fig.add_traces(go.Scatter(
            x=model.alphas_, y=mean_alphas,
            name='Mean', line=dict(color='black', width=3),
            hovertemplate="alpha: %{x} <br>MSE: %{y}",
        ))
        
        fig.add_shape(
            type="line", line=dict(dash='dash'),
            x0=model.alpha_, y0=0,
            x1=model.alpha_, y1=1,
            yref='paper'
        )
        
        fig.update_layout(
            xaxis=dict(
                title=dict(
                    text='alpha'
                ),
                type='log'
            ),
            yaxis=dict(
                title=dict(
                    text='Mean Square Error (MSE)'
                )
            ),
        )
        fig.show()

<img width="3228" height="674" alt="image" src="https://github.com/user-attachments/assets/23e412ce-665f-4f25-b48f-aa4ea3ae79a6" />

Fig 9 Regularisation Plot

Lasso regression with cross-validation is used to predict life expectancy from multiple features. Mean squared error is plotted against the regularisation parameter alpha for each fold. 
The visualisation shows how Lasso penalises large coefficients to prevent overfitting, and selecting an optimal alpha improves model generalisation.

13. Grid Search Visualisation

        from sklearn.model_selection import GridSearchCV
        from sklearn.tree import DecisionTreeRegressor
        
        N_FOLD = 6
        
        # Load and shuffle dataframe
        df = px.data.iris()
        df = df.sample(frac=1, random_state=0)
        
        X = df[['sepal_width', 'sepal_length']]
        y = df['petal_width']
        
        # Define and fit the grid
        model = DecisionTreeRegressor()
        param_grid = {
            'criterion': ['mse', 'friedman_mse', 'mae'],
            'max_depth': range(2, 5)
        }
        grid = GridSearchCV(model, param_grid, cv=N_FOLD)
        grid.fit(X, y)
        grid_df = pd.DataFrame(grid.cv_results_)
        
        # Convert the wide format of the grid into the long format
        # accepted by plotly.express
        melted = (
            grid_df
            .rename(columns=lambda col: col.replace('param_', ''))
            .melt(
                value_vars=[f'split{i}_test_score' for i in range(N_FOLD)],
                id_vars=['mean_test_score', 'mean_fit_time', 'criterion', 'max_depth'],
                var_name="cv_split",
                value_name="r_squared"
            )
        )
        
        # Format the variable names for simplicity
        melted['cv_split'] = (
            melted['cv_split']
            .str.replace('_test_score', '')
            .str.replace('split', '')
        )
        
        # Single function call to plot each figure
        fig_hmap = px.density_heatmap(
            melted, x="max_depth", y='criterion',
            histfunc="sum", z="r_squared",
            title='Grid search results on individual fold',
            hover_data=['mean_fit_time'],
            facet_col="cv_split", facet_col_wrap=3,
            labels={'mean_test_score': "mean_r_squared"}
        )
        
        fig_box = px.box(
            melted, x='max_depth', y='r_squared',
            title='Grid search results ',
            hover_data=['mean_fit_time'],
            points='all',
            color="criterion",
            hover_name='cv_split',
            labels={'mean_test_score': "mean_r_squared"}
        )
        
        # Display
        fig_hmap.show()
        fig_box.show();

<img width="3234" height="1371" alt="image" src="https://github.com/user-attachments/assets/ff9ce13d-036b-47c0-b45b-55e7364acf39" />

Fig 10 GridSearchCV Hyperparameters 

A grid search is performed to tune the hyperparameters of a decision tree regressor. Heatmaps and boxplots visualise the performance of different combinations of parameters across cross-validation folds. These plots help identify the optimal hyperparameters and demonstrate how model performance varies depending on configuration.

Critical Overall Conclusion

Linear regression performs well for simple, linear relationships but cannot capture non-linear trends. kNN and polynomial regression provide flexible modeling options, although higher complexity may lead to overfitting. Multi-feature regression and SVR show the importance of including multiple predictors for accurate predictions.
Regularisation and hyperparameter tuning improve generalisation and prevent overfitting. Interactive visualisations using Plotly and Dash provide intuitive insights into model performance and predictions, making it easier to interpret results and compare models.

# References

Plotly (2026) ML Regression in Python. Available at: https://plotly.com/python/ml-regression/
(Accessed: 28 February 2026).
